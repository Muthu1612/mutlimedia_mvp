import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel  # or Wav2Vec2Model

from transformers import AutoFeatureExtractor
from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer

class Wav2Vec2AudioEncoder():
    
    def __init__(self, dataset, model_checkpoint="facebook/wav2vec2-base", projection_dim=None, freeze_feature_extractor=True):
        super().__init__()
        self.model = AutoModel.from_pretrained(model_checkpoint)
        self.hidden_size = self.model.config.hidden_size
        self.out_dim = projection_dim or self.hidden_size
        if projection_dim is not None:
            self.projector = nn.Linear(self.hidden_size, projection_dim)
        else:
            self.projector = None

        if freeze_feature_extractor:
            # Freeze convolutional feature extractor parameters to save memory/compute
            if hasattr(self.model, "feature_extractor"):
                for p in self.model.feature_extractor.parameters():
                    p.requires_grad = False
                # self.model_checkpoint = model_checkpoint
        
        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)
        self.max_duration = 5.0
        self.encoded_dataset = dataset.map(self._preprocess, remove_columns=["audio"], batched=True)
        self.encoded_dataset.set_format(type="torch")
    
    def forward(self, input_values, attention_mask=None):
        """
        input_values: torch.FloatTensor shape (B, T) or (B, 1, T) depending on model/feature_extractor
        attention_mask: torch.LongTensor shape (B, T_model) matching model output time-steps (optional)
        returns: embeddings (B, out_dim)
        """
        outputs = self.model(input_values, attention_mask=attention_mask, return_dict=True)
        # last_hidden_state: (B, time, hidden)
        hidden = outputs.last_hidden_state

        if attention_mask is not None:
            # attention_mask shape may be (B, seq_len); expand for hidden dims
            mask = attention_mask.unsqueeze(-1).to(hidden.dtype)  # (B, seq_len, 1)
            summed = (hidden * mask).sum(dim=1)  # (B, hidden)
            lengths = mask.sum(dim=1)  # (B, 1)
            pooled = summed / lengths.clamp(min=1e-9)
        else:
            pooled = hidden.mean(dim=1)

        if self.projector is not None:
            pooled = self.projector(pooled)

        return pooled  # (B, out_dim)
    
    def _preprocess(self, examples):
        audio_arrays = [x["array"] for x in examples["audio"]]
        inputs = self.feature_extractor(
            audio_arrays, 
            sampling_rate=self.feature_extractor.sampling_rate, 
            max_length=int(self.feature_extractor.sampling_rate * self.max_duration), 
            truncation=True, 
        )
        return inputs
    
    def get_encoded_dataset(self):
        return self.encoded_dataset
    
    
class Wav2Vec2Classifier(nn.Module):
    def __init__(self, encoder: Wav2Vec2AudioEncoder, num_labels: int, dropout=0.1):
        super().__init__()
        self.encoder = encoder
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(encoder.out_dim, num_labels)

    def forward(self, input_values, attention_mask=None, labels=None):
        emb = self.encoder(input_values, attention_mask=attention_mask)  # (B, D)
        logits = self.classifier(self.dropout(emb))  # (B, num_labels)
        loss = None
        if labels is not None:
            loss = F.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
    


# Example usage:
# num_labels = 2
# encoder = Wav2Vec2AudioEncoder(model_checkpoint=model_checkpoint, projection_dim=None, freeze_feature_extractor=True)
# model = Wav2Vec2Classifier(encoder, num_labels=num_labels)