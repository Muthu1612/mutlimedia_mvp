{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044b9484-06ed-465c-ad9f-364816298e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import gc\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "import torchaudio  # torchaudio==2.9.0  torch==2.9.0 torchcodec==0.8\n",
    "import sys\n",
    "import datasets  # pip install datasets==3.6.0\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e457ce7-6964-4190-95f0-55f6ffac552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee308974b2b4e4e9a6d8cbebb6714aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/53868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6ab320e7c94ddb94fa35a865041ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb088afbb7a42d39242bfced94d2fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2de4c00a3548789e5084a4b2d1fce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/53868 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7580195aa48f428caf5480b2fe0b3b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  71%|#######   | 38077/53868 [00:05<00:02, 7609.25it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3eb3c2579e442058036c98e41b96ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/10798 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2059750dc8e4523ac74266548e00bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/4634 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d197cbe19e7483ca43549936d160b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e795cb5674e64bbfecce3f845f328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39956b2094d4f1886d47c23e6848ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=\"../../data/audio/for-norm/for-norm/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c7f4a9-f942-46d7-a9bd-18564b209bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 53868\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 10798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 4634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0842d4-3abd-4ccb-863c-6c158ecb3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muthu\\miniconda3\\envs\\multi_env\\Lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "encoder = Wav2Vec2Model.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33924358-cd88-4ca2-a08d-5c53585a6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 5.0  # seconds (you can change this)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806c34e7-2213-4423-8370-33cb666027d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "test_only = DatasetDict({\n",
    "    \"test\": full_dataset[\"test\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76cd8ce-7b04-4968-b65a-cd96bda1d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a7a31f67c5478398336f2b70c45aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = test_only.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"audio\",\"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44fa8493-728e-483d-a10c-e24b6c03bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    processed_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator  # <-- important!\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     processed_dataset[\"test\"],\n",
    "#     batch_size=8,\n",
    "#     collate_fn=default_data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f47f5f-8215-4869-96e6-e264382f5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioEmbeddingModel(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if hasattr(self.encoder, \"freeze_feature_extractor\"):\n",
    "            self.encoder.freeze_feature_extractor()\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        outputs = self.encoder(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state  # (B, T_hidden, H)\n",
    "        \n",
    "        # HuggingFace Wav2Vec2 / WavLM already applies attention masking internally\n",
    "        # So you can just do mean pooling over time dimension\n",
    "        embedding = hidden_states.mean(dim=1)     # (B, H)\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c44ab77-4135-4767-a9af-ccb3942819e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEmbeddingModel(\n",
       "  (encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model = AudioEmbeddingModel(encoder).to(device)\n",
    "embedding_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f841a1-e09d-4d7f-b00b-23c7c2ef5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_audio_embs = []\n",
    "# all_labels = []\n",
    "\n",
    "# embedding_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, batch in enumerate(test_loader):\n",
    "#         print(f\"  Processing batch {i+1}/{len(test_loader)}...\")\n",
    "#         input_values = batch[\"input_values\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"]  # still available as tensor\n",
    "\n",
    "#         emb = embedding_model(input_values, attention_mask)\n",
    "#         all_audio_embs.append(emb.cpu())\n",
    "#         all_labels.append(labels)  # collect labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "603dd1ca-1b74-4abf-ab18-556568ac13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embs = torch.cat(all_audio_embs, dim=0)   # (N, H)\n",
    "all_labels = torch.cat(all_labels, dim=0)           # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc03dd88-9420-4f52-acdc-f06b227df9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings/test_audio_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"embeddings/test_audio_embeddings.pt\"\n",
    "torch.save({\n",
    "    \"embeddings\": all_audio_embs,   # (N, D)\n",
    "    \"labels\": all_labels            # (N,)\n",
    "}, save_path)\n",
    "\n",
    "print(\"Saved:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a61fe-0e5c-4c66-88fa-b9507cdd4947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
