{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d2e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import gc\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "import torchaudio  # torchaudio==2.9.0  torch==2.9.0 torchcodec==0.8\n",
    "import sys\n",
    "import datasets  # pip install datasets==3.6.0\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdeaead-c227-48d7-adbf-5ece2b69c2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3c2de39ef400fbadd49edb8f81feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/53868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b39d0988ff4a5c9c12f2a3037390fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329e601298214377a9e13fc3e8f3af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=\"../../data/audio/for-norm/for-norm/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3233ecae-91d7-41a7-ab40-1d66d3f22a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 53868\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 10798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 4634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e091c-57b9-4718-bcfd-6c1d96ff0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "\n",
    "# # Create a DatasetDict with first 20 samples per split\n",
    "# dataset = DatasetDict({\n",
    "#     split: full_dataset[split].select(range(min(20, len(full_dataset[split]))))\n",
    "#     for split in full_dataset.keys()\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96474c43-e13f-4283-b184-bb7694f10d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829aec5-f433-485f-8e7b-1d30f4e6a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "encoder = Wav2Vec2Model.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c42b86-a258-44a3-a9af-0c145423c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 5.0  # seconds (you can change this)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7556c6-ae4c-4683-b149-17194e7a85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"audio\",\"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21adfe6-025b-478b-983a-e5200bd04de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_values', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(processed_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4dc99f8-abe3-4626-9597-e9a5f6466ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator  # <-- important!\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    processed_dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320a25b1-82e2-4f3f-817e-ed3d9ff399c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioEmbeddingModel(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if hasattr(self.encoder, \"freeze_feature_extractor\"):\n",
    "            self.encoder.freeze_feature_extractor()\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        outputs = self.encoder(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state  # (B, T_hidden, H)\n",
    "        \n",
    "        # HuggingFace Wav2Vec2 / WavLM already applies attention masking internally\n",
    "        # So you can just do mean pooling over time dimension\n",
    "        embedding = hidden_states.mean(dim=1)     # (B, H)\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe607d3-8e41-4fe2-bc80-2502933b20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1367: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioEmbeddingModel(\n",
       "  (encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_model = AudioEmbeddingModel(encoder).to(device)\n",
    "embedding_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efac4301-60c7-4ec8-89e0-cca48a091d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embs = []\n",
    "all_labels = []\n",
    "\n",
    "embedding_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]  # still available as tensor\n",
    "\n",
    "        emb = embedding_model(input_values, attention_mask)\n",
    "        all_audio_embs.append(emb.cpu())\n",
    "        all_labels.append(labels)  # collect labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a303cfb-f247-410d-bcb4-268319399758",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embs = torch.cat(all_audio_embs, dim=0)   # (N, H)\n",
    "all_labels = torch.cat(all_labels, dim=0)           # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13be05d1-a9f5-4248-8e8c-68395213c55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0730, -0.0499,  0.1714,  ...,  0.0206,  0.4049, -0.0573],\n",
       "        [ 0.0708, -0.0375,  0.2762,  ...,  0.1415,  0.5486, -0.0400],\n",
       "        [ 0.1824,  0.0113,  0.2098,  ...,  0.0501,  0.4085, -0.1676],\n",
       "        ...,\n",
       "        [ 0.2371,  0.1542, -0.1765,  ..., -0.1527, -0.0872, -0.1793],\n",
       "        [ 0.0206, -0.0475,  0.1065,  ...,  0.1165,  0.4468, -0.1694],\n",
       "        [ 0.0043,  0.0815, -0.0318,  ...,  0.1054,  0.5285, -0.0730]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audio_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d049ffd1-3d06-47f6-9db4-c5d8b33051e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf28ed5-2acc-4fd4-aed6-cfdeb67430b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
