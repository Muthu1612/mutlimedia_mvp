{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5071c93-13c0-4b04-993e-4fe900ddc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import gc\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "import torchaudio  # torchaudio==2.9.0  torch==2.9.0 torchcodec==0.8\n",
    "import sys\n",
    "import datasets  # pip install datasets==3.6.0\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoFeatureExtractor\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32f7eed-36ae-44c5-9409-57934a70066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/wav2vec2-base\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c04d52-d282-4a2a-a4fc-bee89b133e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b754885179540019c9a4b1ef19ffc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fc6156-8e4f-4e1f-a8dd-a411259f783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe2aa0f9224471da599c92c6598e97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/53868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc8bbda7b5244bab7e0cc92fb3b9656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10798 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c238e2e4c14d1e8fe43ec8d8a07998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/4634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"data/for-norm/for-norm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0eb1d3-847b-4067-b1c2-d3f89ae41332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 53868\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 10798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 4634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37383b56-18da-4560-b1e3-1ff2532fedcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'D:\\\\OpenCV\\\\data\\\\for-norm\\\\for-norm\\\\testing\\\\fake\\\\file1078.wav_16k.wav_norm.wav_mono.wav_silence.wav',\n",
       "  'array': array([ 0.12478638,  0.12738037,  0.12841797, ..., -0.13717651,\n",
       "         -0.13647461, -0.1322937 ], shape=(19642,)),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d95cdc7-6ade-4e76-82d9-da9d735e7dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636ad14b923e423eafed3aa27ff4da79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/13956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac9fb074dfd4057949ef5f778033ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299af13ceed54633990eb59e1ec07b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1088 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset2sec = load_dataset(\"audiofolder\", data_dir=\"data/for-2sec/for-2seconds/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35393bed-8aa0-4b1d-903b-ce5b4bcbba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['fake', 'real'], id=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a003468-059e-4cfe-975c-cda2268d14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = dataset[\"train\"].features[\"label\"].names\n",
    "# label2id, pred_label = dict(), dict()\n",
    "# for i, label in enumerate(labels):\n",
    "#     label2id[label] = str(i)\n",
    "#     pred_label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3192705c-ce2c-42bd-ac29-f044592b5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = {'0': 'fake', '1': 'real'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fd9a35-4915-470d-abed-bb9e5c579a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1553cc-8625-45c6-a94e-2dba1e6a1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 5.0  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30578d62-67bc-4bd2-8d11-ba2ce86d1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=feature_extractor.sampling_rate, \n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration), \n",
    "        truncation=True, \n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6175fa-cc28-4738-81e0-2b7e5eaa7a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 53868\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 10798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 4634\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\"], batched=True)\n",
    "encoded_dataset\n",
    "encoded_dataset.set_format(type=\"torch\")\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f488a75-4379-4290-b045-eafc840a5705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320c362e-328e-4962-b441-04599133532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel  # or Wav2Vec2Model\n",
    "# model_checkpoint = \"facebook/wav2vec2-base\"  # already defined in your notebook\n",
    "\n",
    "class Wav2Vec2AudioEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes raw wav input_values (B, T) into pooled embeddings (B, D).\n",
    "    - Uses the pretrained Wav2Vec2 base model.\n",
    "    - Applies masked mean pooling over time using attention_mask if provided.\n",
    "    - Optionally projects to a smaller dimension and can freeze feature extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_checkpoint=\"facebook/wav2vec2-base\", projection_dim=None, freeze_feature_extractor=True):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        self.out_dim = projection_dim or self.hidden_size\n",
    "        if projection_dim is not None:\n",
    "            self.projector = nn.Linear(self.hidden_size, projection_dim)\n",
    "        else:\n",
    "            self.projector = None\n",
    "\n",
    "        if freeze_feature_extractor:\n",
    "            # Freeze convolutional feature extractor parameters to save memory/compute\n",
    "            if hasattr(self.model, \"feature_extractor\"):\n",
    "                for p in self.model.feature_extractor.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        \"\"\"\n",
    "        input_values: torch.FloatTensor shape (B, T) or (B, 1, T) depending on model/feature_extractor\n",
    "        attention_mask: torch.LongTensor shape (B, T_model) matching model output time-steps (optional)\n",
    "        returns: embeddings (B, out_dim)\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_values, attention_mask=attention_mask, return_dict=True)\n",
    "        # last_hidden_state: (B, time, hidden)\n",
    "        hidden = outputs.last_hidden_state\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask shape may be (B, seq_len); expand for hidden dims\n",
    "            mask = attention_mask.unsqueeze(-1).to(hidden.dtype)  # (B, seq_len, 1)\n",
    "            summed = (hidden * mask).sum(dim=1)  # (B, hidden)\n",
    "            lengths = mask.sum(dim=1)  # (B, 1)\n",
    "            pooled = summed / lengths.clamp(min=1e-9)\n",
    "        else:\n",
    "            pooled = hidden.mean(dim=1)\n",
    "\n",
    "        if self.projector is not None:\n",
    "            pooled = self.projector(pooled)\n",
    "\n",
    "        return pooled  # (B, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18be9d56-dc9e-449d-9c9a-7751f4ebe664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2Classifier(nn.Module):\n",
    "    def __init__(self, encoder: Wav2Vec2AudioEncoder, num_labels: int, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(encoder.out_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        emb = self.encoder(input_values, attention_mask=attention_mask)  # (B, D)\n",
    "        logits = self.classifier(self.dropout(emb))  # (B, num_labels)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5cab39c-8a5f-4950-8f4f-49297cef3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels = len(pred_label)\n",
    "# model = AutoModelForAudioClassification.from_pretrained(\n",
    "#     model_checkpoint,\n",
    "#     num_labels=num_labels,\n",
    "#     label2id=label2id,\n",
    "#     pred_label=pred_label,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a9698c-05d9-4336-97ae-c5989bd5b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you create pred_label/label2id and encoded_dataset.set_format(type=\"torch\")\n",
    "num_labels = len(pred_label)\n",
    "encoder = Wav2Vec2AudioEncoder(model_checkpoint=model_checkpoint, projection_dim=None, freeze_feature_extractor=True)\n",
    "model = Wav2Vec2Classifier(encoder, num_labels=num_labels)\n",
    "\n",
    "# Then pass `model` to HuggingFace Trainer; ensure your dataset items contain:\n",
    "# - \"input_values\": torch.FloatTensor (B, T)\n",
    "# - \"attention_mask\": torch.LongTensor (B, T)  <-- optional but recommended\n",
    "# - \"labels\": torch.LongTensor (B,)  (integer class ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc783229-f4d5-40b7-8515-3905c09b1898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values.shape: torch.Size([4, 44073])\n",
      "attention_mask.shape: torch.Size([4, 44073])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = encoded_dataset[\"test\"][0:4]\n",
    "# Suppose batch['input_values'] is a Python list of 1D tensors or numpy arrays\n",
    "seqs = []\n",
    "for v in batch[\"input_values\"]:\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        seqs.append(v)\n",
    "    else:\n",
    "        seqs.append(torch.tensor(v, dtype=torch.float32))\n",
    "\n",
    "# seqs is a list of 1D tensors with different lengths\n",
    "padded = pad_sequence(seqs, batch_first=True, padding_value=0.0)  # (B, T_max)\n",
    "lengths = torch.tensor([s.size(0) for s in seqs], dtype=torch.long)\n",
    "\n",
    "# build attention mask: 1 for real samples, 0 for padding\n",
    "max_len = padded.size(1)\n",
    "mask = (torch.arange(max_len).unsqueeze(0) < lengths.unsqueeze(1)).long()  # (B, T_max)\n",
    "\n",
    "# move to device\n",
    "input_values = padded.to(device)\n",
    "attention_mask = mask.to(device)\n",
    "\n",
    "print(\"input_values.shape:\", input_values.shape)\n",
    "print(\"attention_mask.shape:\", attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f92e94a9-52eb-41f3-8c13-6e9de8a7ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     f\"{model_name}-finetuned-ks\",\n",
    "#     eval_strategy = \"epoch\",\n",
    "#     save_strategy = \"epoch\",\n",
    "#     learning_rate=3e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=5,\n",
    "#     warmup_ratio=0.1,\n",
    "#     logging_steps=10,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"accuracy\",\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24da0607-6567-4aa0-ad54-0be6a62f3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ks\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    # Keep per-device batch small for 4GB GPUs; adjust as needed\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # Enable mixed precision and gradient checkpointing to reduce memory footprint\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fef698f-1e1b-43d3-812c-49e7a6777ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b51d8a1-5304-4a2d-888d-1948892a3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=encoded_dataset[\"train\"],\n",
    "#     eval_dataset=encoded_dataset[\"validation\"],\n",
    "#     processing_class=feature_extractor,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d3b3f41-a8d4-4ddc-b901-28cdfce36bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='488' max='16835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  488/16835 06:03 < 3:23:41, 1.34 it/s, Epoch 0.14/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\accelerate\\accelerator.py:2848\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2847\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2848\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2849\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\my_cv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592a05b-263b-4c3a-9c74-05225b3ac7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
