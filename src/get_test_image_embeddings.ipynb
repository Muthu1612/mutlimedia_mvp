{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691462f7-70f8-4674-8a80-db5e9be3b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and imports\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from loaders.visual_loader import CelebDFVisualDataset\n",
    "from encoders.visual_encoder import VisualEncoder\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from loaders.image_loader import CelebDFImageDataset\n",
    "from encoders.image_encoder import ImageEncoder\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a97e87e-38f6-4326-9d6c-ed6bdd4c764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: D:\\florida_coursework\\third_sem\\multimedia_expert_systems\\multimedia_prototype\\data\\celeb_df\n",
      "Exists: True\n",
      "no. of skipped lines: 0\n"
     ]
    }
   ],
   "source": [
    "# Data path\n",
    "root_dir = Path(r\"D:\\florida_coursework\\third_sem\\multimedia_expert_systems\\multimedia_prototype\\data\\celeb_df\")\n",
    "print(f\"Data directory: {root_dir}\")\n",
    "print(f\"Exists: {data_root.exists()}\")\n",
    "test_list_file = root_dir / \"List_of_testing_videos.txt\"\n",
    "\n",
    "test_videos = []\n",
    "with open(test_list_file, \"r\") as f:\n",
    "    cnt = 0\n",
    "    for line in f:\n",
    "        line = line.strip().strip(\", '[]\")\n",
    "        if not line:\n",
    "            continue\n",
    "        # Each line format: \"1 YouTube-real/00170.mp4\"\n",
    "        try:\n",
    "            lbl_str, rel_path = line.split(maxsplit=1)\n",
    "            lbl = int(lbl_str)\n",
    "            test_videos.append({\n",
    "                \"rel_path\": rel_path,\n",
    "                \"label\": lbl\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            cnt += 1\n",
    "            print(f\"Skipping line due to parsing error: {line} ({e})\")\n",
    "            continue\n",
    "    print(\"no. of skipped lines:\" ,cnt)\n",
    "\n",
    "# convert to DataFrame for easy lookup\n",
    "test_df = pd.DataFrame(test_videos)\n",
    "\n",
    "# gather all videos from dataset folders\n",
    "all_videos = []\n",
    "for subfolder in [\"Celeb-real\", \"Celeb-synthesis\", \"YouTube-real\"]:\n",
    "    folder_path = root_dir / subfolder\n",
    "    for vid in glob(str(folder_path / \"*.mp4\")):\n",
    "        rel_path = f\"{subfolder}/{Path(vid).name}\"\n",
    "\n",
    "        # Determine if video belongs to test set\n",
    "        if rel_path in test_df[\"rel_path\"].values:\n",
    "            tag = \"test\"\n",
    "            label = int(test_df.loc[test_df[\"rel_path\"] == rel_path, \"label\"].values[0])\n",
    "        else:\n",
    "            # Assign label based on folder for train/val\n",
    "            if subfolder in [\"Celeb-real\", \"YouTube-real\"]:\n",
    "                label = 1  # real\n",
    "            else:\n",
    "                label = 0  # fake\n",
    "            tag = \"trainval\"\n",
    "\n",
    "        all_videos.append({\n",
    "            \"path\": vid,\n",
    "            \"rel_path\": rel_path,\n",
    "            \"label\": label,\n",
    "            \"tag\": tag\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_videos)\n",
    "trainval = df[df[\"tag\"] == \"trainval\"]\n",
    "test = df[df[\"tag\"] == \"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628fcac4-780c-4f28-81bf-58461b037d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'D:\\\\florida_coursework\\\\third_sem\\\\multimedia_expert_systems\\\\multimedia_prototype\\\\data\\\\celeb_df\\\\Celeb-real\\\\id0_0001.mp4',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [\n",
    "    {\"path\": row[\"path\"], \"label\": row[\"label\"]}\n",
    "    for _, row in test.iterrows()\n",
    "]\n",
    "test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58dcd836-2332-4225-a153-52586242030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "âœ“ Total videos in dataset: 518\n",
      "  Sample video paths: ['D:\\\\florida_coursework\\\\third_sem\\\\multimedia_expert_systems\\\\multimedia_prototype\\\\data\\\\celeb_df\\\\Celeb-real\\\\id0_0001.mp4', 'D:\\\\florida_coursework\\\\third_sem\\\\multimedia_expert_systems\\\\multimedia_prototype\\\\data\\\\celeb_df\\\\Celeb-real\\\\id10_0001.mp4', 'D:\\\\florida_coursework\\\\third_sem\\\\multimedia_expert_systems\\\\multimedia_prototype\\\\data\\\\celeb_df\\\\Celeb-real\\\\id10_0007.mp4']\n",
      "  Sample labels: [1, 1, 1] (0=real, 1=fake)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load full dataset and create small test subset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = CelebDFImageDataset(root_dir=root_dir, video_list = test_list)\n",
    "\n",
    "print(f\"âœ“ Total videos in dataset: {len(dataset)}\")\n",
    "print(f\"  Sample video paths: {dataset.video_paths[:3]}\")\n",
    "print(f\"  Sample labels: {dataset.labels[:3]} (0=real, 1=fake)\")\n",
    "\n",
    "# # Create small test subset (10 videos)\n",
    "# test_indices = list(range(500))\n",
    "# dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "# print(f\"\\nâœ“ Created test subset: {len(dataset)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f611f0a-74af-4892-ba16-94915cc178c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single sample face extraction...\n",
      "âœ“ Extracted face tensor: torch.Size([3, 224, 224])\n",
      "  Expected shape: (3, 224, 224) - RGB image\n",
      "  Data range: [0.004, 0.941]\n",
      "  Label: 1 (fake)\n",
      "âœ“ Face successfully extracted and preprocessed!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test single sample - face extraction and preprocessing\n",
    "print(\"Testing single sample face extraction...\")\n",
    "sample_img, sample_label = dataset[0]\n",
    "\n",
    "print(f\"âœ“ Extracted face tensor: {sample_img.shape}\")\n",
    "print(f\"  Expected shape: (3, 224, 224) - RGB image\")\n",
    "print(f\"  Data range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")\n",
    "print(f\"  Label: {sample_label.item()} ({'real' if sample_label.item() == 0 else 'fake'})\")\n",
    "\n",
    "# Check if face was detected (non-zero tensor)\n",
    "if sample_img.sum() == 0:\n",
    "    print(\"âš  Warning: Face not detected in this video (zeros returned)\")\n",
    "else:\n",
    "    print(\"âœ“ Face successfully extracted and preprocessed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a07d338-0a8f-43b2-a5a9-d9fb35468f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoader...\n",
      "âœ“ Created DataLoader with 130 batches\n",
      "\n",
      "Batch content:\n",
      "  - Images: torch.Size([4, 3, 224, 224])\n",
      "  - Labels: torch.Size([4]), values: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create DataLoader for batch processing\n",
    "print(\"Creating DataLoader...\")\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0  # Use 0 for Windows to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created DataLoader with {len(test_loader)} batches\")\n",
    "\n",
    "# Test one batch\n",
    "batch_imgs, batch_labels = next(iter(test_loader))\n",
    "print(f\"\\nBatch content:\")\n",
    "print(f\"  - Images: {batch_imgs.shape}\")\n",
    "print(f\"  - Labels: {batch_labels.shape}, values: {batch_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec050a1-dd5d-4f16-80c6-1580b66621b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ImageEncoder (ResNet50)...\n",
      "âœ“ ImageEncoder loaded on cuda\n",
      "  Backbone: ResNet50 (pretrained on ImageNet)\n",
      "  Output dimension: 512\n",
      "  Total parameters: 24,558,144\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize ImageEncoder\n",
    "print(\"Loading ImageEncoder (ResNet50)...\")\n",
    "embed_dim = 512  # Match audio encoder output for multimodal fusion\n",
    "\n",
    "encoder = ImageEncoder(embed_dim=embed_dim).to(device)\n",
    "encoder.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"âœ“ ImageEncoder loaded on {device}\")\n",
    "print(f\"  Backbone: ResNet50 (pretrained on ImageNet)\")\n",
    "print(f\"  Output dimension: {embed_dim}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed54baf-044a-4985-9c2e-90c4478b41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image embeddings...\n",
      "  Batch 1/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 2/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 3/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 4/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 5/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 6/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 7/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 8/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 9/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 10/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 11/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 12/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 13/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 14/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 15/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 16/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 17/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 18/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 19/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 20/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 21/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 22/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 23/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 24/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 25/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 26/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 27/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 28/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 29/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 30/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 31/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 32/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 33/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 34/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 35/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 36/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 37/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 38/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 39/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 40/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 41/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 42/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 43/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 44/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 45/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 46/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 47/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 48/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 49/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 50/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 51/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 52/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 53/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 54/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 55/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 56/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 57/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 58/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 59/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 60/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 61/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 62/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 63/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 64/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 65/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 66/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 67/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 68/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 69/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 70/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 71/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 72/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 73/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 74/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 75/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 76/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 77/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 78/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 79/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 80/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 81/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 82/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 83/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 84/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 85/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 86/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 87/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 88/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 89/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 90/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 91/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 92/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 93/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 94/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 95/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 96/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 97/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 98/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 99/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 100/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 101/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 102/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 103/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 104/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 105/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 106/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 107/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 108/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 109/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 110/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 111/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 112/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 113/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 114/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 115/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 116/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 117/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 118/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 119/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 120/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 121/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 122/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 123/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 124/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 125/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 126/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 127/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 128/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 129/130: images torch.Size([4, 3, 224, 224]) â†’ embeddings torch.Size([4, 512])\n",
      "  Batch 130/130: images torch.Size([2, 3, 224, 224]) â†’ embeddings torch.Size([2, 512])\n",
      "\n",
      "âœ“ Final Results:\n",
      "  Total embeddings: torch.Size([518, 512])\n",
      "  Total labels: torch.Size([518])\n",
      "  Labels distribution: real=340, fake=178\n",
      "\n",
      "ðŸŽ‰ Image embeddings ready for multimodal fusion!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate embeddings for all test batches\n",
    "print(\"Generating image embeddings...\")\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        # Move batch to device\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = encoder(imgs)\n",
    "        \n",
    "        # Store results\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        print(f\"  Batch {i+1}/{len(test_loader)}: \"\n",
    "              f\"images {imgs.shape} â†’ embeddings {embeddings.shape}\")\n",
    "\n",
    "# Concatenate all batches\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "print(f\"\\nâœ“ Final Results:\")\n",
    "print(f\"  Total embeddings: {all_embeddings.shape}\")\n",
    "print(f\"  Total labels: {all_labels.shape}\")\n",
    "print(f\"  Labels distribution: real={(all_labels==0).sum().item()}, fake={(all_labels==1).sum().item()}\")\n",
    "print(f\"\\nðŸŽ‰ Image embeddings ready for multimodal fusion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8beaf4ca-8bab-4c35-872b-59bc273f9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings/test_image_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"embeddings/test_image_embeddings.pt\"\n",
    "torch.save({\n",
    "    \"embeddings\": all_embeddings,   # (N, D)\n",
    "    \"labels\": all_labels            # (N,)\n",
    "}, save_path)\n",
    "\n",
    "print(\"Saved:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f355aa-84ec-4cfc-8e2b-246a7e1cbba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
